{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5254bacc",
   "metadata": {},
   "source": [
    "# üìä Customer Churn Prediction ‚Äî Google Colab Notebook\n",
    "**Covers**: Data prep, EDA, CHAID-like tree (DecisionTree with entropy), Logistic Regression, ROC-AUC, Lift/Gain, Rule extraction, Deployment (joblib).  \n",
    "**Dataset**: Telco Customer Churn (Kaggle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2bfd7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 32] The process cannot access the file because it is being used by another process: 'C:\\\\Users\\\\harip\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\Lib\\\\site-packages\\\\numpy\\\\_core\\\\tests\\\\test_scalarbuffer.py'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n",
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m get_ipython().system(\u001b[33m'\u001b[39m\u001b[33mpip -q install scikit-learn pandas numpy matplotlib joblib\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# === Setup: Install & Imports ===\n",
    "# Note: Install packages using: pip install scikit-learn pandas numpy matplotlib joblib\n",
    "# Or uncomment the line below if running in Jupyter with pip available\n",
    "# !pip install scikit-learn pandas numpy matplotlib joblib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, roc_auc_score, roc_curve, confusion_matrix, classification_report\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, export_text\n",
    "import joblib\n",
    "\n",
    "import os\n",
    "# Create necessary directories\n",
    "os.makedirs(\"charts\", exist_ok=True)\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "os.makedirs(\"reports\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450f0e19",
   "metadata": {},
   "source": [
    "## üì• Load the dataset\n",
    "Choose **one** of the options below:\n",
    "- **A)** Upload `telco_customer_churn.csv` from your computer\n",
    "- **B)** Load from Google Drive (if you've placed it there)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabc9253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION A: Load from local file system\n",
    "# Update the path below to point to your telco_customer_churn.csv file\n",
    "import os\n",
    "\n",
    "# Option 1: If CSV is in the same directory as this notebook\n",
    "csv_path = \"telco_customer_churn.csv\"\n",
    "\n",
    "# Option 2: If CSV is in a specific directory, uncomment and update:\n",
    "# csv_path = r\"C:\\path\\to\\your\\telco_customer_churn.csv\"\n",
    "\n",
    "# Option 3: Auto-detect if in Downloads folder\n",
    "if not os.path.exists(csv_path):\n",
    "    downloads_path = os.path.join(os.path.expanduser(\"~\"), \"Downloads\", \"telco_customer_churn.csv\")\n",
    "    if os.path.exists(downloads_path):\n",
    "        csv_path = downloads_path\n",
    "        print(f\"Found CSV in Downloads: {csv_path}\")\n",
    "    else:\n",
    "        print(f\"CSV not found. Please update csv_path variable with the correct path to your dataset.\")\n",
    "        csv_path = None\n",
    "else:\n",
    "    print(f\"Using CSV: {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baf3833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION B: Alternative - Load from a different path\n",
    "# If Option A didn't work, you can set csv_path directly here:\n",
    "# csv_path = r\"C:\\Users\\harip\\Downloads\\telco_customer_churn.csv\"\n",
    "\n",
    "# Check if csv_path is set\n",
    "if csv_path is None or not os.path.exists(csv_path):\n",
    "    print(\"‚ö†Ô∏è CSV file not found!\")\n",
    "    print(\"‚û°Ô∏è Please update `csv_path` in the previous cell (Option A) with the correct path to your dataset.\")\n",
    "    print(\"   The file should be named 'telco_customer_churn.csv'\")\n",
    "else:\n",
    "    print(f\"‚úÖ Using CSV: {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915c61fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Helper functions ===\n",
    "def load_telco(csv_path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # Drop ID if present\n",
    "    if \"customerID\" in df.columns:\n",
    "        df = df.drop(columns=[\"customerID\"])\n",
    "    # Numeric conversion quirks\n",
    "    if \"TotalCharges\" in df.columns:\n",
    "        df[\"TotalCharges\"] = pd.to_numeric(df[\"TotalCharges\"], errors=\"coerce\")\n",
    "    # Drop rows with null target\n",
    "    df = df.dropna(subset=[\"Churn\"])\n",
    "    # Strip column names\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    return df\n",
    "\n",
    "def train_test_prepare(df: pd.DataFrame, target_col=\"Churn\", positive_label=\"Yes\"):\n",
    "    y = (df[target_col].astype(str).str.strip() == positive_label).astype(int)\n",
    "    X = df.drop(columns=[target_col])\n",
    "\n",
    "    # Identify categorical vs numeric\n",
    "    cat_cols = X.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "    num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    # Preprocessor\n",
    "    pre = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "            (\"num\", \"passthrough\", num_cols)\n",
    "        ]\n",
    "    )\n",
    "    return X, y, pre, cat_cols, num_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e305238",
   "metadata": {},
   "source": [
    "## üîç EDA (Exploratory Data Analysis)\n",
    "Basic insights and churn distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdeab2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "assert csv_path is not None, \"Set csv_path to your dataset path first.\"\n",
    "df = load_telco(csv_path)\n",
    "print(\"Shape:\", df.shape)\n",
    "display(df.head())\n",
    "\n",
    "# Churn distribution\n",
    "churn_counts = df['Churn'].value_counts()\n",
    "print(\"\\nChurn distribution:\\n\", churn_counts)\n",
    "churn_counts.plot(kind=\"bar\", title=\"Churn Distribution\")\n",
    "plt.xlabel(\"Churn\"); plt.ylabel(\"Count\"); plt.tight_layout(); plt.savefig(\"charts/churn_distribution.png\"); plt.show()\n",
    "\n",
    "# Example relationship: tenure vs churn\n",
    "if \"tenure\" in df.columns:\n",
    "    df.boxplot(column=\"tenure\", by=\"Churn\")\n",
    "    plt.title(\"Tenure by Churn\"); plt.suptitle(\"\"); plt.tight_layout(); plt.savefig(\"charts/tenure_by_churn.png\"); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405916da",
   "metadata": {},
   "source": [
    "## ü§ñ Modeling: Logistic Regression & CHAID-like Tree\n",
    "We'll train and compare two models:\n",
    "- **Logistic Regression**\n",
    "- **Decision Tree (entropy)** as a **CHAID-like** proxy and extract readable rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f456f81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare splits and preprocessing\n",
    "X, y, pre, cat_cols, num_cols = train_test_prepare(df, target_col=\"Churn\", positive_label=\"Yes\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "\n",
    "# Logistic Regression pipeline\n",
    "logit = Pipeline(steps=[\n",
    "    (\"pre\", pre),\n",
    "    (\"clf\", LogisticRegression(max_iter=1000, n_jobs=None))\n",
    "])\n",
    "\n",
    "# Decision Tree (CHAID-like via entropy)\n",
    "tree = Pipeline(steps=[\n",
    "    (\"pre\", pre),\n",
    "    (\"clf\", DecisionTreeClassifier(criterion=\"entropy\", max_depth=5, random_state=42))\n",
    "])\n",
    "\n",
    "# Fit models\n",
    "logit.fit(X_train, y_train)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_logit = logit.predict(X_test)\n",
    "y_proba_logit = logit.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_pred_tree = tree.predict(X_test)\n",
    "y_proba_tree = tree.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "def summarize(y_true, y_pred, y_proba):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    auc = roc_auc_score(y_true, y_proba)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    return acc, auc, cm\n",
    "\n",
    "acc_l, auc_l, cm_l = summarize(y_test, y_pred_logit, y_proba_logit)\n",
    "acc_t, auc_t, cm_t = summarize(y_test, y_pred_tree, y_proba_tree)\n",
    "\n",
    "print(\"Logistic Regression ‚Äî Accuracy:\", round(acc_l,4), \"AUC:\", round(auc_l,4), \"\\nConfusion Matrix:\\n\", cm_l)\n",
    "print(\"\\nDecision Tree ‚Äî Accuracy:\", round(acc_t,4), \"AUC:\", round(auc_t,4), \"\\nConfusion Matrix:\\n\", cm_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c9a98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve comparison\n",
    "fpr_l, tpr_l, _ = roc_curve(y_test, y_proba_logit)\n",
    "fpr_t, tpr_t, _ = roc_curve(y_test, y_proba_tree)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr_l, tpr_l, label=f\"Logit AUC={roc_auc_score(y_test, y_proba_logit):.3f}\")\n",
    "plt.plot(fpr_t, tpr_t, label=f\"Tree AUC={roc_auc_score(y_test, y_proba_tree):.3f}\")\n",
    "plt.plot([0,1],[0,1], linestyle=\"--\")\n",
    "plt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\"); plt.title(\"ROC Curves\")\n",
    "plt.legend(); plt.tight_layout(); plt.savefig(\"charts/roc_comparison.png\"); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00622dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lift & Gains table\n",
    "def lift_gain_table(y_true, y_proba, bins=10):\n",
    "    df2 = pd.DataFrame({\"y\": y_true, \"p\": y_proba}).sort_values(\"p\", ascending=False).reset_index(drop=True)\n",
    "    df2[\"decile\"] = pd.qcut(df2.index + 1, bins, labels=False, duplicates=\"drop\") + 1\n",
    "    agg = df2.groupby(\"decile\").apply(lambda g: pd.Series({\n",
    "        \"n\": len(g),\n",
    "        \"positives\": g[\"y\"].sum(),\n",
    "        \"avg_p\": g[\"p\"].mean()\n",
    "    })).reset_index()\n",
    "    total_pos = df2[\"y\"].sum()\n",
    "    agg[\"cum_pos\"] = agg[\"positives\"].cumsum()\n",
    "    agg[\"cum_rate\"] = agg[\"cum_pos\"] / total_pos if total_pos > 0 else 0\n",
    "    agg[\"perc_pop\"] = agg[\"n\"].cumsum() / len(df2)\n",
    "    agg[\"gain\"] = agg[\"cum_rate\"]\n",
    "    agg[\"lift\"] = agg[\"gain\"] / agg[\"perc_pop\"]\n",
    "    return agg\n",
    "\n",
    "lg_logit = lift_gain_table(y_test, y_proba_logit)\n",
    "lg_tree  = lift_gain_table(y_test, y_proba_tree)\n",
    "\n",
    "print(\"=== Lift/Gain (Logit) ===\")\n",
    "display(lg_logit)\n",
    "print(\"=== Lift/Gain (Tree) ===\")\n",
    "display(lg_tree)\n",
    "\n",
    "# Simple Lift plot (Top deciles)\n",
    "plt.figure()\n",
    "plt.plot(lg_logit[\"decile\"], lg_logit[\"lift\"], marker=\"o\", label=\"Logit\")\n",
    "plt.plot(lg_tree[\"decile\"], lg_tree[\"lift\"], marker=\"o\", label=\"Tree\")\n",
    "plt.xlabel(\"Decile (1=Top scored)\"); plt.ylabel(\"Lift\"); plt.title(\"Lift Chart\")\n",
    "plt.legend(); plt.tight_layout(); plt.savefig(\"charts/lift_chart.png\"); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789182c4",
   "metadata": {},
   "source": [
    "## üìú Rule Extraction (from the Decision Tree)\n",
    "This gives you readable rules similar to CHAID splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc92290d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export readable tree (on the transformed feature space)\n",
    "dt = tree.named_steps[\"clf\"]\n",
    "ohe = tree.named_steps[\"pre\"].named_transformers_[\"cat\"]\n",
    "num_cols = tree.named_steps[\"pre\"].transformers_[1][2]  # numeric passthrough\n",
    "ohe_features = ohe.get_feature_names_out(ohe.feature_names_in_)\n",
    "all_features = list(ohe_features) + list(num_cols)\n",
    "\n",
    "rules_text = export_text(dt, feature_names=all_features, decimals=2)\n",
    "print(rules_text)\n",
    "\n",
    "# Save rules to reports\n",
    "with open(\"reports/decision_tree_rules.txt\", \"w\") as f:\n",
    "    f.write(rules_text)\n",
    "print(\"Rules saved to reports/decision_tree_rules.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b6a547",
   "metadata": {},
   "source": [
    "## üíæ Save Best Model & üîÆ Inference Demo\n",
    "This saves the better AUC model to `models/best_model.pkl` and shows how to predict for one new customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fe8c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model, best_name, best_auc = (logit, \"logistic_regression\", auc_l) if auc_l >= auc_t else (tree, \"decision_tree_entropy\", auc_t)\n",
    "joblib.dump(best_model, \"models/best_model.pkl\")\n",
    "print(f\"Saved best model: {best_name} with AUC={best_auc:.4f}\")\n",
    "\n",
    "# Inference example (edit values to a real row schema)\n",
    "sample = X_test.iloc[[0]].copy()\n",
    "proba = best_model.predict_proba(sample)[:, 1][0]\n",
    "pred = int(proba >= 0.5)\n",
    "print(\"Sample predicted churn probability:\", round(proba, 4), \" Pred label:\", pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd8a560",
   "metadata": {},
   "source": [
    "## üìù Report-ready Summaries\n",
    "Copy these into your PDF report (Model Comparison and Evaluation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29e1563",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = {\n",
    "    \"logistic_regression\": {\n",
    "        \"accuracy\": round(accuracy_score(y_test, y_pred_logit), 4),\n",
    "        \"auc\": round(roc_auc_score(y_test, y_proba_logit), 4),\n",
    "        \"confusion_matrix\": confusion_matrix(y_test, y_pred_logit).tolist()\n",
    "    },\n",
    "    \"decision_tree\": {\n",
    "        \"accuracy\": round(accuracy_score(y_test, y_pred_tree), 4),\n",
    "        \"auc\": round(roc_auc_score(y_test, y_proba_tree), 4),\n",
    "        \"confusion_matrix\": confusion_matrix(y_test, y_pred_tree).tolist()\n",
    "    }\n",
    "}\n",
    "summary"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Churn_Prediction_Colab.ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
